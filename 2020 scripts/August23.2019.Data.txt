DBM paper:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3656686/

# The 1000 Genome project is real data
# The DBM paper seemed to have generated data from the 1000 Genome project

3 RESULTS
3.1 Simulation from human data
We evaluated the performance of DBM using datasets generated
from human sequences with European (CEU) and African (YRI)
origins. We downloaded the phased haplotypes of CEU and YRI
individuals from the 1000 Genomes Project (The 1000 Genomes
Project Consortium, 2010). Using these reference haplotypes, we
simulated haplotypes of new individuals and their corresponding
read counts as follows: (i) we generated new haplotypes as
mosaic combination of reference haplotypes with transition
rate 1 per 200 kb, and reference individuals are randomly
chosen; (ii) we randomly paired new haplotypes to form new
individuals; (iii) we simulated read counts at each SNP using a
Poisson distribution with mean x/2, where  denotes the
sequencing coverage and x ¼ 0,1,2 denotes the allele count; (iv)
we generated random sequencing errors in read counts across the
genome at rate 0.01 per basepair per read; this created both falsepositive alleles and false-positive SNPs; and (v) we removed all
reads carrying alleles that are different from the two most frequent alleles at each SNP, as they are most likely sequencing
errors; we also removed SNPs whose minor read count is less
than a threshold, such that the total number of false-positive
SNPs in the data is controlled <5%.




3.4 Using reference input
DBM can further take input of reference genotypes and haplotypes. To use reference genotypes, one can simply convert
genotypes to pseudo read counts. For genotypes AA, Aa and
aa, the read counts can be written as (2X,0), (X,X) and (0,2X),
respectively, with large X (e.g.  15). If the reference data are in
haplotype format, we treat each reference haplotype as an ‘individual’, and we fit each reference haplotype with one Markov
chain only. To do this, we replace the observation probability
Pr(DjZ) in formula (2) by an indicator function Iz¼a at each
SNP, with ‘a’ denoting the true allele in the current haplotype
at each SNP.
To evaluate the benefit of using extra data in genotype calling
and haplotype phasing, we simulated 40 individuals (CEU and
YRI, respectively) at 10 000 SNPs with sequencing depth  ¼ 3
and sequencing error rate 0.01. These are the sample individuals.
We further simulated additional 40 individuals (CEU and YRI,
respectively) as the references. We ran DBM to analyse the 40
sample individuals along with various numbers of references
input to the program. The references were input in three ways:
(i) in form of read counts at sequencing depth  ¼ 3, which then
merely increased the sample size; (ii) in form of known
genotypes, which eliminated genotyping uncertainties in the reference data; and (iii) in form of known haplotypes, which further
provided phasing information in the reference data





HapSeq2 paper: 
https://www.ncbi.nlm.nih.gov/pubmed/23943637


# HapSeq2 seems to also use the 1000 Genome project and simulation data
Through simulations generated from the HapMap data (International HapMap Consortium, 2007), we
show that our method significantly outperforms traditional imputation methods. We show that using our
method, haplotypes can be accurately inferred using 1X coverage assuming a 0.01 error rate. Hap-seq
achieved a 2.79% switch error rate, which is a 15% improvement over the number of switch errors over the
state-of-the-art haplotype imputation algorithm. In addition, our method is the first practical method for
predicting haplotypes for rare variants.

...

4. EXPERIMENTAL RESULTS
We perform simulation experiments to compare the performance of our Hap-seq algorithm and the
standard HMM. The implementations of the two methods are as follows. The standard HMM is our own
implementation of the IMPUTE v1.0 model, which uses the pre-defined genetic map information for the
transition probability. We use the genetic map data downloaded from the IMPUTE website. Since IMPUTE
does not accept sequence read data, we allow our own implementation to accept sequence read by splitting
the reads into SNP-wise information similarly to MACH (Li et al., 2010). Our implementation runs the
Viterbi algorithm and gives the most likely haplotype phasing based on the IMPUTE model. To make a fair
comparison, for the part of Hap-seq that computes the imputation likelihood, we use the same IMPUTE
v1.0 model using the same genetic map data.
We use 60 parental individuals of CEU population of HapMap Phase II data downloaded from the
HapMap website. We perform a leave-one-out experiment to measure the accuracy of our method. We
choose one target individual, generate simulated sequence reads, and infer the genotypes and phasing of the
target individual using the 59 individuals as the reference data set. We repeat this for all 60 individuals and
the results are averaged. When we generate the data, we use the per-allele sequencing error rate of 1%. We
assume that both IMPUTE and Hap-seq know the true error rate. In all datasets we generate, we assume a
low coverage of 1x. That is, all heterozygous sites will be covered by two reads on average, since there are
two chromosomes


# TODO: access these papers
HapSeq2 is cited by 3 papers on PubMed
DBM is cited by 211 papers on PubMed
https://www.ncbi.nlm.nih.gov/pubmed?cmd=link&linkname=pubmed_pubmed&uid=23407359&log$=relatedarticles&logdbfrom=pmc

